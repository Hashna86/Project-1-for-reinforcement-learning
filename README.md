# Introduction
## Part 1: Stationary k-Armed Bandit
### Problem Description
We consider a 10-armed bandit problem where the true action values (means) for each arm are drawn from a normal distribution 
ğ‘
(
0
,
1
)
N(0,1), and the rewards are drawn from 
ğ‘
(
ğœ‡
ğ‘–
,
1
)
N(Î¼ 
i
â€‹
 ,1). The objective is to evaluate different bandit algorithms in terms of:

Average reward over time.

Percentage of optimal action selection.

We run each algorithm over 2000 time steps for 1000 simulations, each with independent random seeds.

Algorithms Implemented
Greedy: Exploits only, with initial 
ğ‘„
=
0
Q=0.

Epsilon-Greedy: Explores with probability 
ğœ–
Ïµ. Tuned via pilot run; best Îµ = 0.1.

Optimistic Greedy: Greedy with high initial Q-values (99.5th percentile).

Gradient Bandit: Uses preferences and softmax action selection. Best Î± = 0.2 (chosen via pilot).

Tuning and Evaluation
Epsilon-greedy: Tested several Îµ values from 0.01 to 0.4; Îµ = 0.1 yielded consistent high rewards and optimality.

Optimistic Greedy: Initial Q-values set using 99.5th percentile of 
ğ‘
(
ğœ‡
ğ‘š
ğ‘
ğ‘¥
,
1
)
N(Î¼ 
max
â€‹
 ,1).

Gradient Bandit: Pilot tested Î± values [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]; Î± = 0.2 found best balance.
